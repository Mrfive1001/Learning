{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度下降法(Gradient descent)\n",
    "梯度下降法是求解无约束最优化问题的一种最常用的方法，它是一种迭代法，每一步都需要求解目标函数的梯度向量。![](http://images2015.cnblogs.com/blog/1042406/201610/1042406-20161017221342935-1872962415.png)\n",
    "假设问题如下：$$\\min_x f(x)$$求解步骤如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、选取初值x0  \n",
    "2、计算梯度、计算函数值   \n",
    "3、当梯度小于某个范围时，停止迭代，否则$x(k+1) = x(k) - \\lambda p(x(k))$,p(x(k))是函数梯度方向   \n",
    "4、计算f(x(k+1))与f(x(k))的差值，小于某个范围，停止迭代"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中关于步长，可以是定步长，也可以是变步长。定步长时由于梯度绝对值在下降，因此速度会减慢。\n",
    "定步长很可能一步就走过了‘谷底’，优化速度变慢，或者不准确。  \n",
    "变步长用的是一维线性搜索：  \n",
    "精确一维搜索：求出$f(x_k+\\lambda_k p_k) = \\min f(x_k +\\lambda p_k)$，可以使用二分搜索以及其他的搜索方法。  \n",
    "不精确一维搜索：选取一个可以接受的下降量，使得$f(x_k)-f(x_k+\\lambda_k p_k)>0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优缺点：  \n",
    "梯度下降法和最小二乘法相比，梯度下降法需要选择步长，而最小二乘法不需要。梯度下降法是迭代求解，最小二乘法是计算解析解。如果样本量不算很大，且存在解析解，最小二乘法比起梯度下降法要有优势，计算速度很快。但是如果样本量很大，用最小二乘法由于需要求一个超级大的逆矩阵，这时就很难或者很慢才能求解解析解了，使用迭代的梯度下降法比较有优势。   \n",
    "梯度下降法和牛顿法/拟牛顿法相比，两者都是迭代求解，不过梯度下降法是梯度求解，而牛顿法/拟牛顿法是用二阶的海森矩阵的逆矩阵或伪逆矩阵求解。相对而言，使用牛顿法/拟牛顿法收敛更快。但是每次迭代的时间比梯度下降法长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 牛顿法\n",
    "牛顿法也是求解无约束最优化的常用方法，优点在于二次拟合，求解速度快。缺点需要求海森阵，计算较为复杂。下面是数学推导："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设f(x)有二阶偏导数，假设某次迭代值为x(k),可以将f(x)在此处二阶泰勒展开，对于单独变量是：$$  f(x) = f(x^k)+f^{'}(x^k)(x-x^k)+\\frac{1}{2}f^{''}(x^k)(x-x^k)^2$$对f(x)求梯度，并使其为0得：$$ \\triangledown f(x) = f^{'}(x^k)+f^{''}(x^k)(x-x^k) = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "则可以得到下一个迭代值为：$$x^{k+1} =x^k - \\frac{f^{'}(x^k)}{f^{''}(x^k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "求解步骤:根据梯度决定是否需要跳出循环，根据上式进行迭代。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若是多变量的情况，需要将其中的一阶导数和二阶导数分别换成雅可比矩阵和海参阵。  \n",
    "牛顿法优化的物理含义是利用二阶导数来不断迭代来接近最优值，比较好的就是，不需要选取步长，速度较快，但计算比较麻烦。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  拟牛顿法\n",
    "拟牛顿法是对牛顿法进行改进的一种方法，牛顿法由于需要计算海森阵的逆矩阵，因此计算比较复杂，所以拟牛顿法的思想就是找到一个n阶矩阵来近似代替海森阵的逆矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DFP算法\n",
    "#### BFGS算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 共轭梯度方法 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "共轭梯度法是介于最速下降法和牛顿法之间的一个方法，仅需要一阶导数，但下降收敛快，且不用计算海森阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本概念：  \n",
    "共轭若两个向量关于一个正定对称矩阵满足式子X'AY = 0，称这两个向量共轭。  \n",
    "对于A这个n*n的矩阵，能够找到n个共轭的独立的非零向量。所以找正定二次函数的极值点，只需要找到沿着n个方向的极值即可。   \n",
    "根据我的理解，共轭法梯度法指的是在搜索的时候加入负梯度搜索的方法。正定二次函数的极小值问题如下$$\\min f(x) = \\frac{1}{2}X'AX+B'X+c$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "具体算法不需要了解。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
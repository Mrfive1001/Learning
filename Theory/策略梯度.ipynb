{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 策略梯度\n",
    "策略梯度方法是强化学习中基于策略的一种方法，这种方法的基本思想是，假设我们有一个策略pi，输入状态，输出动作或者每个动作对应的概率。其中：\n",
    "* 直接输出某一动作的称为**确定性策略梯度**\n",
    "* 输出一个动作分布概率的称为**随机性策略梯度**\n",
    "\n",
    "有了策略之后，我们规定某个指标对这个策略进行评价,然后按照评价的梯度进行策略改进。根据Shulman的博士论文，在保持策略梯度不变的情况下，策略梯度可写为：\n",
    "![](http://www.zhihu.com/equation?tex=%5C%5B+g%3D%5Cmathbb%7BE%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%7B%5CPsi_t%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28a_t%7Cs_t%5Cright%29%7D%5Cright%5D+%5C%5D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个公式可以理解为，当对某一(s,a)评价函数为正时，总回报函数的改变方向和a出现概率改变的方向一致，即，当a出现概率增加时，策略进行了改善。其中的评价函数可以取：\n",
    "\n",
    "* 轨迹的总回报\n",
    "\n",
    "* 动作的即时回报\n",
    "\n",
    "* 状态行为值函数Q\n",
    "\n",
    "* 优势函数A\n",
    "\n",
    "* TD残差r+V(s2)-V(s1)\n",
    "\n",
    "1-2：直接应用轨迹的累计回报，策略梯度无偏差，但由于需要积累多步回报，方差较大。  \n",
    "\n",
    "3-5：利用动作值函数，优势函数和TD偏差来代替几类回报，方差较小，但是这都需要逼近方法，计算的策略梯度有偏差。这三个都需要使用AC框架。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## AC框架\n",
    "\n",
    "Actor指的是行动者，也就是策略，Critc指的是评价函数，用来估计前面那个评价函数。框架的迭代过程类似于GPI（通用策略迭代）方法，首先是策略评估，然后策略改进。其中Crtic表示策略评估，Actor更新的时候进行策略改进。其中Actor、Critic经常用神经网络来表示，称为策略网络、评价网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "[Actor-Crtic小结](https://zhuanlan.zhihu.com/p/29486661)\n",
    "\n",
    "\n",
    "[李宏毅](https://www.youtube.com/channel/UC2ggjtuuWvxrHHHiaDH1dlQ/videos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
